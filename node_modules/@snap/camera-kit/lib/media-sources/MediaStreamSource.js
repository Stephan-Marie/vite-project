import { __awaiter } from "tslib";
import { copyDefinedProperties } from "../common/copyDefinedProperties";
import { ensureError } from "../common/errorHelpers";
import { Transform2D } from "../transforms/Transform2D";
import { CameraKitSource, defaultDeviceInfo } from "./CameraKitSource";
const defaultOptions = Object.assign(Object.assign({}, defaultDeviceInfo), { transform: Transform2D.Identity, disableSourceAudio: false });
function closeWorklet(worklet) {
    if (!worklet)
        return;
    worklet.port.close();
    worklet.port.onmessage = null;
    worklet.disconnect();
}
function closeAudioContext(audioContext) {
    return __awaiter(this, void 0, void 0, function* () {
        if (!audioContext || audioContext.state === "closed")
            return;
        return audioContext.close();
    });
}
/**
 * Create a {@link CameraKitSource} from any
 * [MediaStream](https://developer.mozilla.org/en-US/docs/Web/API/MediaStream).
 *
 * @param stream Any MediaStream, such as obtained via `canvas.captureStream()` or `mediaDevices.getUserMedia()`.
 * @param options Options.
 *
 * @category Rendering
 */
export function createMediaStreamSource(stream, options = {}) {
    var _a;
    const { facingMode } = stream.getVideoTracks().length > 0 ? stream.getVideoTracks()[0].getSettings() : { facingMode: undefined };
    const detectedCameraType = facingMode === "user" || facingMode === "environment" ? facingMode : undefined;
    const optionsWithDefaults = Object.assign(Object.assign(Object.assign({}, defaultOptions), copyDefinedProperties(options)), { cameraType: (_a = options.cameraType) !== null && _a !== void 0 ? _a : detectedCameraType });
    const enableSourceAudio = stream.getAudioTracks().length > 0 && !optionsWithDefaults.disableSourceAudio;
    const simulateStereoAudio = true;
    const sampleRate = 44100;
    let audioContext = undefined;
    let audioSource = undefined;
    let worklet = undefined;
    let microphoneRecorderUrl;
    if (enableSourceAudio) {
        // https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_AudioWorklet
        const microphoneRecorderWorklet = `
        class MicrophoneWorkletProcessor extends AudioWorkletProcessor {
            process(inputs, outputs, parameters) {
                this.port.postMessage({
                    eventType: 'data',
                    buffer: inputs
                });
                return true;
            }
        }
        registerProcessor('microphone-worklet', MicrophoneWorkletProcessor);`;
        const microphoneRecorderBlob = new Blob([microphoneRecorderWorklet], {
            type: "application/javascript",
        });
        microphoneRecorderUrl = URL.createObjectURL(microphoneRecorderBlob);
    }
    return new CameraKitSource({ media: stream }, {
        onAttach: (source, lensCore, reportError) => __awaiter(this, void 0, void 0, function* () {
            yield source.setTransform(optionsWithDefaults.transform);
            if (enableSourceAudio) {
                // Audio paramters set has to be called before lens is applied
                yield lensCore.setAudioParameters({
                    parameters: {
                        numChannels: simulateStereoAudio ? 2 : 1,
                        sampleRate,
                    },
                });
                try {
                    // There is a possibility of the onAttach method being called twice in a row due to a bug.
                    // To ensure there are not leaks, it is better to close any existing connections.
                    closeWorklet(worklet);
                    audioSource === null || audioSource === void 0 ? void 0 : audioSource.disconnect();
                    yield closeAudioContext(audioContext);
                }
                catch (error) {
                    // We still want to continue if anything above failed
                    reportError(ensureError(error));
                }
                audioContext = new AudioContext();
                audioSource = audioContext.createMediaStreamSource(stream);
                const scopedAudioSource = audioSource;
                audioContext.audioWorklet
                    .addModule(microphoneRecorderUrl)
                    .then(() => {
                    if (audioContext) {
                        worklet = new AudioWorkletNode(audioContext, "microphone-worklet");
                        scopedAudioSource.connect(worklet);
                        worklet.connect(audioContext.destination);
                        // NOTE: We subscribe to messages here, and they will continue to arrive
                        // even after audioContext.close() is called. To disconnect the audio worklets
                        // created here, we need to track two variables - worklet and audioSource.
                        // By calling disconnect() on them, we can properly
                        // disconnect the audio worklets.
                        worklet.port.onmessage = (e) => {
                            if (e.data.eventType === "data") {
                                // developer.mozilla.org/en-US/docs/Web/API/AudioWorkletProcessor/process
                                // inputs[n][m] is the list of samples in the n-th input at the m-th channel.
                                const leftSamples = e.data.buffer[0][0];
                                // Firefox might have leftSamples undefined:
                                // https://jira.sc-corp.net/browse/CAMKIT-5189
                                if (!leftSamples)
                                    return;
                                let inputBuffers = [leftSamples];
                                if (simulateStereoAudio) {
                                    const rightSamples = e.data.buffer[0].length > 1 ? e.data.buffer[0][1] : leftSamples.slice();
                                    inputBuffers.push(rightSamples);
                                }
                                lensCore.processAudioSampleBuffer({ input: inputBuffers }).catch(reportError);
                            }
                        };
                    }
                })
                    .catch((error) => {
                    reportError(error);
                });
            }
        }),
        onDetach: (reportError) => __awaiter(this, void 0, void 0, function* () {
            if (worklet) {
                closeWorklet(worklet);
                worklet = undefined;
            }
            if (audioSource) {
                audioSource.disconnect();
                audioSource = undefined;
            }
            if (audioContext) {
                yield closeAudioContext(audioContext).catch(reportError);
                audioContext = undefined;
            }
        }),
    }, optionsWithDefaults);
}
//# sourceMappingURL=MediaStreamSource.js.map